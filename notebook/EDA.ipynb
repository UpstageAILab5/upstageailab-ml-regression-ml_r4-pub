{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root found: d:\\dev\\upstageailab5-ml-regression-ml_r4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import pygwalker as pyg\n",
    "import dabl\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "#### For Path setup\n",
    "def setup_project_path():\n",
    "    \"\"\"프로젝트 루트 경로를 찾아서 파이썬 경로에 추가\"\"\"\n",
    "    current = Path.cwd()\n",
    "    while current != current.parent:\n",
    "        if (current / '.git').exists():\n",
    "            if str(current) not in sys.path:\n",
    "                sys.path.append(str(current))\n",
    "                print(f'Project root found: {current}')\n",
    "            return current\n",
    "        current = current.parent\n",
    "    return None\n",
    "\n",
    "# 프로젝트 경로 설정\n",
    "project_root = setup_project_path()\n",
    "if project_root is None:\n",
    "    # 프로젝트 루트를 찾지 못했다면 직접 지정\n",
    "    project_root = Path(\"D:/dev/upstageailab5-ml-regression-ml_r4\")\n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.append(str(project_root))\n",
    "\n",
    "#jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-08 23:12:10,603 - root - INFO - Initialized Logger.\n",
      "2024-11-08 23:12:10,605 - root - INFO - Windows platform. Font: ['Malgun Gothic']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-08 23:12:13,663 - root - INFO - #### Current workspalce: d:\\dev\\upstageailab5-ml-regression-ml_r4\n",
      "2024-11-08 23:12:13,664 - root - INFO - Windows platform. Path: D:\\dev\\upstageailab5-ml-regression-ml_r4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.logger import Logger\n",
    "from src.preprocessing import DataPrep\n",
    "from src.eda import EDA\n",
    "from src.feature import FeatureEngineer, Clustering, XAI\n",
    "from src.train import Model\n",
    "from src.visualization import Visualizer\n",
    "from src.utils import Utils, PathManager\n",
    "## memory management\n",
    "import gc\n",
    "gc.collect()\n",
    "########################################################################################################################################\n",
    "logger_instance = Logger()\n",
    "logger = logger_instance.logger\n",
    "utils = Utils(logger)\n",
    "utils.setup_font_and_path_platform()\n",
    "current_platform = utils.current_platform\n",
    "#os.environ['PYTHONPATH'] = r'D:\\dev\\upstageailab5-ml-regression-ml_r4'\n",
    "current_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "logger.info(f'#### Current workspalce: {current_path}')\n",
    "if current_platform == 'Windows':\n",
    "    base_path = Path(r'D:\\dev\\upstageailab5-ml-regression-ml_r4')\n",
    "    logger.info(f'{current_platform} platform. Path: {base_path}')\n",
    "elif current_platform == 'Darwin':          # Mac\n",
    "    base_path = Path('/data/ephemeral/home/dev/upstageailab5-ml-regression-ml_r4')\n",
    "    logger.info(f'{current_platform} platform. Path: {base_path}')\n",
    "else:\n",
    "    base_path = Path('/data/ephemeral/home/dev/upstageailab5-ml-regression-ml_r4')    # Linux\n",
    "    logger.info(f'{current_platform} platform. Path: {base_path}')\n",
    "########################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PathManager' object has no attribute 'paths'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pm \u001b[38;5;241m=\u001b[39m \u001b[43mPathManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m pm\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[1;32md:\\dev\\upstageailab5-ml-regression-ml_r4\\src\\utils.py:214\u001b[0m, in \u001b[0;36mPathManager.__init__\u001b[1;34m(self, base_path)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_path \u001b[38;5;241m=\u001b[39m Path(base_path)\u001b[38;5;241m.\u001b[39mresolve()  \u001b[38;5;66;03m# resolve()로 절대 경로 변환\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mos_type \u001b[38;5;241m=\u001b[39m platform\u001b[38;5;241m.\u001b[39msystem()  \u001b[38;5;66;03m# 'Windows', 'Linux', 'Darwin' (Mac)\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpaths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_subdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    221\u001b[0m report_path \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_subdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreport\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\dev\\upstageailab5-ml-regression-ml_r4\\src\\utils.py:242\u001b[0m, in \u001b[0;36mPathManager.add_paths\u001b[1;34m(self, paths_config)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# 디렉토리 생성\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     full_path\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 242\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpaths\u001b[49m[name] \u001b[38;5;241m=\u001b[39m full_path\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpaths\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PathManager' object has no attribute 'paths'"
     ]
    }
   ],
   "source": [
    "pm = PathManager(base_path)\n",
    "pm.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-08 22:59:19,725 - root - INFO - #### Init Data Prep.. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # # 기본 경로 생성\n",
    "# paths = pm.add_paths({\n",
    "#     'logs': 'logs',\n",
    "#     'config': 'config',\n",
    "#     'output': 'output',\n",
    "#     'data': 'data'\n",
    "# })\n",
    "\n",
    "# # 하위 디렉토리 생성\n",
    "# model_path = pm.create_subdir('output', 'models')\n",
    "# report_path =pm.create_subdir('output', 'report')\n",
    "# prep_data_path = pm.create_subdir('data', 'processed')\n",
    "\n",
    "# cache_path = pm.create_subdir('data', 'cache')\n",
    "\n",
    "# # 경로 사용\n",
    "# log_path = pm.get_path('logs')\n",
    "# config_path = pm.get_path('config')\n",
    "pm.get_all_paths()\n",
    "\n",
    "# path -data  \n",
    "\n",
    "\n",
    "\n",
    "config ={'base_path':base_path,\n",
    "        #'out_path':pm.get_path('output'),\n",
    "        'subway_feature': os.path.join(base_path, 'data','subway_feature.csv'),\n",
    "        'bus_feature': os.path.join(base_path, 'data','bus_feature.csv'),\n",
    "        'logger': logger_instance,#logger,\n",
    "        'random_seed': 2024,\n",
    "     \n",
    "        'wandb': {\n",
    "            'project': 'project-regression_house_price',     # 필수: wandb 프로젝트명\n",
    "            'entity': 'joon',          # 필수: wandb 사용자/조직명\n",
    "            'group': 'group-ml4',    # 선택: 실험 그룹명\n",
    "        }\n",
    "    }\n",
    "\n",
    "config.update(loaded_config)\n",
    "########################################################################################################################################\n",
    "### Data Prep\n",
    "\n",
    "path_baseline = os.path.join(prep_data_path, 'df_baseline_prep.csv')\n",
    "path_auto = os.path.join(prep_data_path, 'df_auto_prep.csv')\n",
    "########################################################################################################################################\n",
    "### EDA\n",
    "path_feat = os.path.join(prep_data_path, 'df_feat.csv')\n",
    "path_feat_add = os.path.join(prep_data_path, 'df_feat_add.csv')\n",
    "\n",
    "data_prep = DataPrep(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=os.path.join(model_path, 'data')\n",
    "print(aa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.get_all_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list_csv = Utils.list_files(prep_path, '.csv')\n",
    "#pprint.pprint(list_csv)\n",
    "path_csv = list_csv[6]\n",
    "print(path_csv)\n",
    "df= pd.read_csv(path_csv)\n",
    "df.head()\n",
    "df.columns\n",
    "df = utils.remove_unnamed_columns(df)\n",
    "df.columns\n",
    "\n",
    "col_id=['is_test','target']\n",
    "cols_to_select = ['강남여부', '신축여부','건축면적', '주차대수','아파트명','전용면적','계약월','계약년','층','건축년도','도로명','k-단지분류(아파트,주상복합등등)', 'k-세대타입(분양형태)', 'k-관리방식', 'k-복도유형', 'k-난방방식', 'k-전체동수', 'k-전체세대수', 'k-건설사(시공사)', 'k-시행사', 'k-연면적', 'k-주거전용면적', 'k-관리비부과면적', 'k-전용면적별세대현황(60㎡이하)','k-전용면적별세대현황(60㎡~85㎡이하)', 'k-85㎡~135㎡이하','경비비관리형태', '세대전기계약방법', '청소비관리형태']\n",
    "\n",
    "col_feat=df.columns[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_selected = df[cols_to_select+list(col_feat)]\n",
    "df_id=df[col_id]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Prep Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "# 숫자형 데이터에 대해 변환\n",
    "from sklearn.preprocessing import RobustScaler, PowerTransformer\n",
    "\n",
    "def classify_columns(df, unique_threshold=10, freq_threshold=0.8, entropy_threshold=1.0):\n",
    "    numeric_cols = []\n",
    "    categorical_cols = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        # Check if column is numeric\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            unique_values = df[col].nunique()\n",
    "            value_counts = df[col].value_counts(normalize=True)\n",
    "            col_entropy = entropy(value_counts)\n",
    "\n",
    "            # Classification logic\n",
    "            if unique_values <= unique_threshold:\n",
    "                categorical_cols.append(col)\n",
    "            elif any(value_counts > freq_threshold):\n",
    "                categorical_cols.append(col)\n",
    "            elif col_entropy < entropy_threshold:\n",
    "                categorical_cols.append(col)\n",
    "            else:\n",
    "                numeric_cols.append(col)\n",
    "        else:\n",
    "            categorical_cols.append(col)\n",
    "    print(f'Numeric columns:{len(numeric_cols)} \\n{numeric_cols}')\n",
    "    print(f'Categorical columns:{len(categorical_cols)} \\n{categorical_cols}')\n",
    "\n",
    "    return numeric_cols, categorical_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols, categorical_cols =classify_columns(df_selected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols, categorical_cols = classify_columns(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import PowerTransformer, RobustScaler\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "\n",
    "@Utils.timeit\n",
    "def scale_numeric_features(df):\n",
    "    \"\"\"\n",
    "    Applies Yeo-Johnson or Box-Cox transform based on the values.\n",
    "    Includes error handling and fallback to RobustScaler.\n",
    "    \"\"\"\n",
    "    print('Scaling numeric features')\n",
    "    transformed_df = df.copy()\n",
    "    \n",
    "    for col in tqdm(df.select_dtypes(include='number').columns):\n",
    "        try:\n",
    "            # 상수값 체크\n",
    "            if df[col].nunique() == 1:\n",
    "                print(f\"Warning: Column '{col}' has constant values. Skipping transformation.\")\n",
    "                continue\n",
    "                \n",
    "            # 0이나 음수가 있는지 체크\n",
    "            if (df[col] > 0).all():\n",
    "                transformer = PowerTransformer(method='box-cox', standardize=True)\n",
    "            else:\n",
    "                transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "            \n",
    "            # 이상치나 무한값 처리\n",
    "            data = df[[col]].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "            if len(data) == 0:\n",
    "                print(f\"Warning: Column '{col}' has no valid data after cleaning. Skipping transformation.\")\n",
    "                continue\n",
    "                \n",
    "            # 변환 시도\n",
    "            transformed_values = transformer.fit_transform(data)\n",
    "            transformed_df.loc[data.index, col] = transformed_values\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to transform '{col}' using PowerTransformer: {str(e)}\")\n",
    "            print(f\"Falling back to RobustScaler for '{col}'\")\n",
    "            try:\n",
    "                # RobustScaler로 대체\n",
    "                scaler = RobustScaler()\n",
    "                transformed_df[col] = scaler.fit_transform(df[[col]])\n",
    "            except Exception as e2:\n",
    "                print(f\"Warning: Failed to scale '{col}' using RobustScaler: {str(e2)}\")\n",
    "                print(f\"Keeping original values for '{col}'\")\n",
    "    \n",
    "    return transformed_df\n",
    "\n",
    "@Utils.timeit\n",
    "def calculate_correlation(df, corr_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Identifies groups of correlated features based on a specified threshold.\n",
    "    \"\"\"\n",
    "    print(f'Calculating correlation with threshold: {corr_threshold}')\n",
    "    \n",
    "    # 숫자형 컬럼만 선택\n",
    "    numeric_df = df.select_dtypes(include='number')\n",
    "    \n",
    "    # 상관관계 계산\n",
    "    corr_matrix = numeric_df.corr().abs()\n",
    "    correlated_groups = {}\n",
    "\n",
    "    # NaN 값 처리\n",
    "    corr_matrix = corr_matrix.fillna(0)\n",
    "\n",
    "    for i in tqdm(range(len(corr_matrix.columns))):\n",
    "        for j in range(i):\n",
    "            if corr_matrix.iloc[i, j] > corr_threshold:\n",
    "                col1 = corr_matrix.columns[i]\n",
    "                col2 = corr_matrix.columns[j]\n",
    "                if col1 not in correlated_groups:\n",
    "                    correlated_groups[col1] = {col1}\n",
    "                correlated_groups[col1].add(col2)\n",
    "    print(f'Found {len(correlated_groups)} groups of correlated features.\\n{correlated_groups}')\n",
    "    return correlated_groups\n",
    "\n",
    "@Utils.timeit\n",
    "def select_important_features(df, correlated_groups):\n",
    "    \"\"\"\n",
    "    Selects the most important feature from correlated groups based on mutual information.\n",
    "    \"\"\"\n",
    "    print(f'Selecting important features from {len(correlated_groups)} groups')\n",
    "    selected_features = set()\n",
    "    numeric_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "    # 타겟 변수 선택 (첫 번째 컬럼 대신 특정 컬럼 지정)\n",
    "    target_col = numeric_cols[0]  # 또는 특정 컬럼 지정\n",
    "    \n",
    "    # Calculate feature importance\n",
    "    importance_scores = {}\n",
    "    for col in tqdm(numeric_cols):\n",
    "        if col == target_col:\n",
    "            continue\n",
    "        try:\n",
    "            # NaN 값 처리\n",
    "            X = df[[col]].fillna(df[col].mean())\n",
    "            y = df[target_col].fillna(df[target_col].mean())\n",
    "            importance_scores[col] = mutual_info_regression(X, y)[0]\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to calculate importance for '{col}': {str(e)}\")\n",
    "            importance_scores[col] = 0\n",
    "\n",
    "    # Select features\n",
    "    for group in tqdm(correlated_groups.values()):\n",
    "        group = list(group)\n",
    "        group = sorted(group, key=lambda x: importance_scores.get(x, 0), reverse=True)\n",
    "        if group:  # 그룹이 비어있지 않은 경우만\n",
    "            selected_features.add(group[0])\n",
    "\n",
    "    # Add uncorrelated features\n",
    "    uncorrelated_features = set(numeric_cols) - {item for group in correlated_groups.values() for item in group}\n",
    "    selected_features.update(uncorrelated_features)\n",
    "    print(f'Found {len(uncorrelated_features)} uncorrelated features: {uncorrelated_features}')\n",
    "    return list(selected_features)\n",
    "@Utils.timeit\n",
    "def preprocess_and_select(df, corr_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Applies scaling, calculates correlation, and selects important features.\n",
    "    \"\"\"\n",
    "    # Step 1: Scale numeric features\n",
    "    scaled_df = scale_numeric_features(df)\n",
    "\n",
    "    # Step 2: Calculate correlated groups\n",
    "    correlated_groups = calculate_correlation(scaled_df, corr_threshold=corr_threshold)\n",
    "\n",
    "    # Step 3: Select important features\n",
    "    selected_features = select_important_features(scaled_df, correlated_groups)\n",
    "    print(f'Selected {len(selected_features)} features: {selected_features}')\n",
    "\n",
    "    return scaled_df[selected_features]\n",
    "\n",
    "processed_df = preprocess_and_select(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.to_csv(os.path.join(prep_path, 'df_fest_selected.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = os.path.join(base_path, 'data')\n",
    "list_csv_data = Utils.list_files(path_data, '.csv')\n",
    "print(list_csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_csv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PygWalker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = 'df_feat_selected'\n",
    "html_path = os.path.join(out_path,f\"eda_pygwalker_{df_name}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pygwalker\n",
    "df_name = 'df_feat'\n",
    "walker = pyg.walk(\n",
    "    df_selected,\n",
    "    spec=os.path.join(out_path,f\"eda_pygwalker_{df_name}.json\"),    # this json file will save your chart state, you need to click save button in ui mannual when you finish a chart, 'autosave' will be supported in the future.\n",
    "    kernel_computation=True,          # set `kernel_computation=True`, pygwalker will use duckdb as computing engine, it support you explore bigger dataset(<=100GB).\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(out_path,f\"eda_pygwalker_{df_name}.html\")\n",
    "print(path)\n",
    "walker.to_html(os.path.join(out_path,f\"eda_pygwalker_{df_name}.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_str = pyg.to_html(df_selected)\n",
    "with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ydata profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# 방법 1: 설정 딕셔너리 사용\n",
    "profile_config = {\n",
    "    \"title\": \"yData Profiling Report: House Price Prediction\",\n",
    "    \"dataset\": {\n",
    "        \"description\": \"House Price Prediction\",\n",
    "        \"copyright_holder\": \"CC0: Public Domain\",\n",
    "        \"url\": \"\"\n",
    "    },\n",
    "    \"correlations\": {\n",
    "        \"pearson\": {\"calculate\": True},\n",
    "        \"spearman\": {\"calculate\": True},\n",
    "        \"kendall\": {\"calculate\": False},\n",
    "        \"phi_k\": {\"calculate\": False}\n",
    "    },\n",
    "    \"plot\": {\n",
    "        \"wordcloud\": {\n",
    "            \"enabled\": True,\n",
    "            \"max_words\": 100\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "profile = ProfileReport(df_selected, **profile_config)\n",
    "\n",
    "# 방법 2: 직접 설정\n",
    "profile = ProfileReport(\n",
    "    df,\n",
    "    title=\"yData Profiling Report: House Price Prediction\",\n",
    "    dataset={\n",
    "        \"description\": \"House Price Prediction\",\n",
    "        \"copyright_holder\": \"CC0: Public Domain\",\n",
    "        \"url\": \"\"\n",
    "    },\n",
    "    correlations={\n",
    "        \"pearson\": {\"calculate\": True},\n",
    "        \"spearman\": {\"calculate\": True},\n",
    "        \"kendall\": {\"calculate\": False},\n",
    "        \"phi_k\": {\"calculate\": False}\n",
    "    },\n",
    "    plot={\n",
    "        \"wordcloud\": {\n",
    "            \"enabled\": True,\n",
    "            \"max_words\": 100\n",
    "        }\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 결과 출력\n",
    "profile.to_widgets() # ~15 minutes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_file( os.path.join(out_path, \"y_data_profiling_df_1.html\"))\n",
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "loaded_config = Utils.load_nested_yaml('config.yaml')\n",
    "sweep_configs = Utils.get_nested_value(loaded_config, 'sweep_configs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = df.sample(frac=0.1, random_state=1) \n",
    "\n",
    "config={'out_path':os.path.join(base_path, 'output')}\n",
    "data_prep = DataPrep(config)\n",
    "df =data_prep.prep_null(df)\n",
    "df.shape\n",
    "\n",
    "df = data_prep.prep_inter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.columns = df.columns.str.replace('[-/]~', '', regex=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataPrep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprep.eda import create_report\n",
    "from dataprep.eda import plot\n",
    "\n",
    "# EDA 보고서 생성\n",
    "report = create_report(df)\n",
    "report.show_browser()\n",
    "\n",
    "# 특정 컬럼에 대한 상세 분석\n",
    "plot(df, \"column_name\")\n",
    "# 두 변수 간의 관계 분석\n",
    "plot(df, \"column1\", \"column2\")\n",
    "\n",
    "# # 결측값 처리\n",
    "# from dataprep.clean import clean_missing\n",
    "# from dataprep.clean import clean_outliers\n",
    "# df_cleaned = clean_missing(df, missing_num='mean', missing_cat='mode')\n",
    "# df_no_outliers = clean_outliers(df, method='zscore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweetviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sweetviz as sv\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# 단일 데이터셋 분석\n",
    "report = sv.analyze(df)\n",
    "report.show_html('report.html')\n",
    "\n",
    "# 두 데이터셋 비교 (예: 훈련셋과 테스트셋)\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "compare_report = sv.compare([train_df, \"Train\"], [test_df, \"Test\"])\n",
    "compare_report.show_html('comparison_report.html')\n",
    "\n",
    "# 타겟 변수 지정 및 특정 설정 적용\n",
    "report_with_target = sv.analyze(df, target_feat='target_column', \n",
    "                                feat_cfg=sv.FeatureConfig(skip=['column_to_skip'], \n",
    "                                                         force_text=['column_as_text']))\n",
    "report_with_target.show_html('report_with_target.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_df = SmartDataframe(df)\n",
    "smart_df.chat('Which column features are critical to predict house price? Convert putative numeric values as float or integer.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_df.chat('Do Basic EDA and explain it.')\n",
    "\n",
    "#\n",
    "# 시군구\t번지\t본번\t부번\t아파트명\t전용면적(㎡)\t계약년월\t계약일\t층\t건축년도\t도로명\t해제사유발생일\t등기신청일자\t거래유형\t중개사소재지\tk-단지분류(아파트,주상복합등등)\n",
    "# 0\t서울특별시 종로구 옥인동\t\t4933.0\t307.0\tDMC래미안클라시스\t71.84\t202004\t23\t40\t1998\t마들로 646\t20200917.0\t20230914\t중개거래\t서울 강북구, 서울 성동구\t도시형 생활주택(아파트)\n",
    "# 1\t서울특별시 성북구 성북동\t44-8\tNaN\tNaN\t래미안하이리버\t15.94\t201709\t8\t20\t2022\t남부순환로248길 78\tNaN\t20230915\t직거래\t서울 광진구, 서울 송파구\tNone\n",
    "# 2\t서울특별시 용산구 효창동\t1312\t1293.0\t2000.0\tNone\t112.23\t201702\t4\t8\t1978\t이촌로54길 5\t20230325.0\t20230413\t-\t경기 김포시, 서울 강서구\t연립주택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_df.chat('Classify Two groups: Categorical or Numeric. Print total number of each group, and each columns. plot basic EDA stuff, based on the sorting (feature importance). Including correlation, heatmap.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_df.chat('Select important features, and suggest proper scaling method for each different features, for house price (numerica) prediction problem.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_df.chat('Select important features and tell me why, for house price (numerica) prediction problem.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_df.chat('Plot distribution and scatter plot altogher. if possible, 3D plot.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_df.chat('Plot distribution and scatter plot altogher. if possible, 3D plot.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_df.chat('Plot 3 most import figures.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_df.chat('What kind of scaling is needed for important features, each?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(df)\n",
    "agent.chat('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.chat('Plot the histogram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from main_baseline import DataPrep, EDA, FeatureEngineer, Model\n",
    "# import pygwalker as pyg\n",
    "\n",
    "\n",
    "data_prep = DataPrep(config)\n",
    "feat_eng = FeatureEngineer(config)\n",
    "model_instance = Model(config)\n",
    "df = data_prep.load_data(base_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_prep.prep_null(df)\n",
    "df = data_prep.prep_inter(df)\n",
    "# 위 방법으로 전용 면적에 대한 이상치를 제거해보겠습니다.\n",
    "cols = ['계약년', '전용면적', '강남여부', '구', '건축년도', '좌표X', '좌표Y', '동']\n",
    "df = data_prep.remove_outliers_iqr(df, '전용면적')\n",
    "# 이상치 제거 후의 shape은 아래와 같습니다. 약 10만개의 데이터가 제거된 모습을 확인할 수 있습니다.\n",
    "print(df.shape)\n",
    "df['is_test'].value_counts()     # 또한, train data만 제거되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "walker = pyg.walk(\n",
    "    df,\n",
    "    spec=\"./chart_meta_0.json\",    # this json file will save your chart state, you need to click save button in ui mannual when you finish a chart, 'autosave' will be supported in the future.\n",
    "    kernel_computation=True,          # set `kernel_computation=True`, pygwalker will use duckdb as computing engine, it support you explore bigger dataset(<=100GB).\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Feat eng\n",
    "df = feat_eng.prep_feat(df)\n",
    "df_coor = {'x': '좌표X', 'y': '좌표Y'}\n",
    "df_subway = data_prep.load_feat_data(os.path.join(base_path, 'data','subway_feature.csv'))\n",
    "df_bus = data_prep.load_feat_data(os.path.joing(base_path, 'data','bus_feature.csv'))\n",
    "subway_coor = {'x': '위도', 'y': '경도'}\n",
    "bus_coor = {'x': 'X좌표', 'y': 'Y좌표'}\n",
    "\n",
    "df = feat_eng.sum_distances_from_a_to_b(df, df_coor , df_subway, subway_coor, 'subway')\n",
    "df = feat_eng.sum_distances_from_a_to_b(df, df_coor , df_bus, bus_coor, 'bus')\n",
    "### split\n",
    "dt_train, dt_test, continuous_columns_v2, categorical_columns_v2 = feat_eng.split_train_test(df)\n",
    "dt_train, label_encoders = feat_eng.encode_label(dt_train, dt_test, continuous_columns_v2, categorical_columns_v2)\n",
    "X_train, X_val, y_train, y_val = feat_eng.split_dataset(dt_train)\n",
    "\n",
    "prep_data = {'X_train': X_train,\n",
    "            'X_val': X_val,\n",
    "            'y_train': y_train,\n",
    "            'y_val': y_val,\n",
    "            'continuous_columns': continuous_columns_v2,\n",
    "            'categorical_columns': categorical_columns_v2\n",
    "}\n",
    "out_path_data = model_instance.save_data(prep_data)\n",
    "# loaded_data = load_data_pkl(out_path_data)\n",
    "# print(loaded_data)\n",
    "model, pred = model_instance.model_train(X_train, X_val, y_train, y_val)\n",
    "feat_eng.select_var(model, X_val, y_val, pred, label_encoders, categorical_columns_v2)\n",
    "model_instance.inference(dt_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
