{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(os.path.join(prep_path, 'df_encoded.csv'), index_col=0)\n",
    "################# Feature Categorical & Numerical\n",
    "cateogirical_features_add = ['강남여부', '신축여부','구', '동']\n",
    "categorical_features = ['k-건설사(시공사)', 'k-관리방식', 'k-난방방식', 'k-단지분류(아파트,주상복합등등)', 'k-복도유형', 'k-사용검사일-사용승인일', 'k-세대타입(분양형태)', 'k-수정일자', 'k-시행사', '경비비관리형태', '관리비 업로드', '기타/의무/임대/임의=1/2/3/4', '단지승인일', '단지신청일', '도로명', '번지', '사용허가여부', '세대전기계약방법',  '아파트명', '청소비관리형태']+cateogirical_features_add\n",
    "\n",
    "# 시군구 제외\n",
    "numerical_features_add = ['계약년', '계약월']\n",
    "numerical_features = ['k-85㎡~135㎡이하', 'k-관리비부과면적', 'k-연면적', 'k-전용면적별세대현황(60㎡~85㎡이하)', 'k-전용면적별세대현황(60㎡이하)', 'k-전체동수', 'k-전체세대수', 'k-주거전용면적', '건축년도', '건축면적', '계약일', '본번', '부번', '전용면적', '좌표X', '좌표Y', '주차대수', '층']+ numerical_features_add\n",
    "\n",
    "#assert len(categorical_features) + len(numerical_features) +2== df.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Dict\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def concat_train_test(dt, dt_test):\n",
    "    Utils.remove_unnamed_columns(dt)\n",
    "    Utils.remove_unnamed_columns(dt_test)\n",
    "    dt['is_test'] = 0\n",
    "    dt_test['is_test'] = 1\n",
    "    dt_test['target'] = 0\n",
    "    concat = pd.concat([dt, dt_test], axis=0).reset_index(drop=True)\n",
    "    print(concat['is_test'].value_counts())\n",
    "\n",
    "    return concat\n",
    "\n",
    "def unconcat_train_test(concat):\n",
    "    Utils.remove_unnamed_columns(concat)\n",
    "    dt = concat.query('is_test==0')\n",
    "    # y_train = dt['target']\n",
    "    dt.drop(columns=['is_test'], inplace=True)\n",
    "    dt_test = concat.query('is_test==1')\n",
    "    dt_test.drop(columns=['target', 'is_test'], inplace=True)\n",
    "    return dt, dt_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataScaler:\n",
    "    def __init__(self, \n",
    "                 target_col: str = None,\n",
    "                 categorical_cols: List[str] = None,\n",
    "                 continuous_cols: List[str] = None,\n",
    "                 exclude_cols: List[str] = None):\n",
    "        \"\"\"\n",
    "        데이터 스케일링을 위한 클래스\n",
    "        \n",
    "        Args:\n",
    "            target_col: 타겟 컬럼명 (스케일링에서 제외)\n",
    "            categorical_cols: 범주형 변수 리스트\n",
    "            continuous_cols: 연속형 변수 리스트\n",
    "            exclude_cols: 스케일링에서 제외할 컬럼 리스트\n",
    "        \"\"\"\n",
    "        self.target_col = target_col\n",
    "        self.categorical_cols = set(categorical_cols) if categorical_cols else set()\n",
    "        self.continuous_cols = set(continuous_cols) if continuous_cols else set()\n",
    "        self.exclude_cols = set(exclude_cols) if exclude_cols else set()\n",
    "        self.scalers = {}\n",
    "        \n",
    "    def _get_appropriate_scaler(self, col_name: str, data: pd.Series) -> object:\n",
    "        \"\"\"\n",
    "        컬럼 특성에 맞는 스케일러 반환\n",
    "        \n",
    "        Args:\n",
    "            col_name: 컬럼명\n",
    "            data: 스케일링할 데이터\n",
    "            \n",
    "        Returns:\n",
    "            스케일러 객체\n",
    "        \"\"\"\n",
    "        # 범주형 변수는 StandardScaler 사용\n",
    "        # if col_name in self.categorical_cols:\n",
    "        #     return StandardScaler()\n",
    "        \n",
    "        # 연속형 변수는 왜도에 따라 스케일러 결정\n",
    "        if abs(data.skew()) > 1:\n",
    "            # 심한 왜도는 RobustScaler + PowerTransformer\n",
    "            return Pipeline([\n",
    "                ('robust', RobustScaler()),\n",
    "                ('power', PowerTransformer(method='yeo-johnson'))\n",
    "            ])\n",
    "        \n",
    "        # 나머지는 RobustScaler\n",
    "        return RobustScaler()\n",
    "        \n",
    "    def scale_features(self, df: pd.DataFrame, is_train: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        특성별 적절한 스케일링 적용\n",
    "        \n",
    "        Args:\n",
    "            df: 스케일링할 데이터프레임\n",
    "            is_train: 학습 데이터 여부\n",
    "            \n",
    "        Returns:\n",
    "            스케일링된 데이터프레임\n",
    "        \"\"\"\n",
    "        scaled_df = df.copy()\n",
    "        \n",
    "        # 스케일링할 컬럼 결정\n",
    "        if self.continuous_cols or self.categorical_cols:\n",
    "            # 지정된 컬럼이 있는 경우\n",
    "            scale_cols = self.continuous_cols | self.categorical_cols\n",
    "        else:\n",
    "            # 지정된 컬럼이 없는 경우 수치형 컬럼 자동 선택\n",
    "            scale_cols = set(df.select_dtypes(include=['int64', 'float64']).columns)\n",
    "        \n",
    "        # 제외할 컬럼 처리\n",
    "        exclude_set = self.exclude_cols.copy()\n",
    "        if self.target_col:\n",
    "            exclude_set.add(self.target_col)\n",
    "        exclude_set.add('is_test')  # 항상 제외\n",
    "        \n",
    "        scale_cols = scale_cols - exclude_set\n",
    "        \n",
    "        # 실제 존재하는 컬럼만 선택\n",
    "        actual_scale_cols = scale_cols & set(df.columns)\n",
    "        \n",
    "        # 누락된 컬럼 확인\n",
    "        missing_cols = scale_cols - actual_scale_cols\n",
    "        if missing_cols:\n",
    "            print(f\"Warning: 다음 컬럼들이 데이터에 없습니다: {sorted(missing_cols)}\")\n",
    "        \n",
    "        print(f\"스케일링 적용 컬럼: {sorted(actual_scale_cols)}\")\n",
    "        \n",
    "        # 스케일링 수행\n",
    "        for col in tqdm(actual_scale_cols, desc='Scaling columns'):\n",
    "            if is_train:\n",
    "                # 학습 데이터: 스케일러 생성 및 적용\n",
    "                scaler = self._get_appropriate_scaler(col, scaled_df[col])\n",
    "                self.scalers[col] = scaler\n",
    "                scaled_values = scaler.fit_transform(scaled_df[[col]]).ravel()\n",
    "                scaled_df[col] = scaled_values\n",
    "            else:\n",
    "                # 테스트 데이터: 기존 스케일러 적용\n",
    "                if col in self.scalers:\n",
    "                    scaled_values = self.scalers[col].transform(scaled_df[[col]]).ravel()\n",
    "                    scaled_df[col] = scaled_values\n",
    "                else:\n",
    "                    print(f\"Warning: {col} 컬럼의 스케일러가 없습니다.\")\n",
    "        \n",
    "        return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = unconcat_train_test(df) # train, test data 분리\n",
    "\n",
    "# # categorical, numerical 기준은 baseline 으로 임시 작성. 변경 가능.\n",
    "# categorical_features =['전용면적', '계약일', '층', '건축년도', 'k-전체동수', 'k-전체세대수', 'k-연면적', 'k-주거전용면적', 'k-관리비부과면적', 'k-전용면적별세대현황(60㎡이하)', 'k-전용면적별세대현황(60㎡~85㎡이하)', 'k-85㎡~135㎡이하', '건축면적', '주차대수', '좌표X', '좌표Y', 'target', '강남여부', '신축여부']\n",
    "# numerical_features = ['번지', '본번', '부번', '아파트명', '도로명', 'k-단지분류(아파트,주상복합등등)', 'k-전화번호', 'k-팩스번호', 'k-세대타입(분양형태)', 'k-관리방식', 'k-복도유형', 'k-난방방식', 'k-건설사(시공사)', 'k-시행사', 'k-사용검사일-사용승인일', 'k-수정일자', '고용보험관리번호', '경비비관리형태', '세대전기계약방법', '청소비관리형태', '기타/의무/임대/임의=1/2/3/4', '단지승인일', '사용허가여부', '관리비 업로드', '단지신청일', '구', '동', '계약년', '계약월']\n",
    "# add_features = ['구', '동', '계약년', '계약월']\n",
    "# numerical_features = list(set(numerical_features)-set(add_features))\n",
    "\n",
    "data_scaler = DataScaler(target_col='target',\n",
    "                            categorical_cols=categorical_features,\n",
    "                            continuous_cols=numerical_features,\n",
    "                            exclude_cols=['is_test']  ) # 아직 \n",
    "\n",
    "# 1. Numerical Features 들의 경우, outlier removal 없이 이상치에 강건하게 Robust Scaling 을 먼저 적용\n",
    "# 학습 데이터 전처리\n",
    "train_scaled = data_scaler.scale_features(df_train[numerical_features], is_train=True)\n",
    "# 테스트 데이터 전처리 (학습 데이터의 스케일러 사용)\n",
    "test_scaled = data_scaler.scale_features(df_test[numerical_features], is_train=False)\n",
    "\n",
    "\n",
    "# 2. Categorical Features 들의 경우, Feature engineering 후 encoding 까지 완료된 이후에 표준화 스케일링을 별도 적용합니다\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled.isnull().sum()\n",
    "train_scaled[categorical_features] = df_train[categorical_features]\n",
    "test_scaled[categorical_features] = df_test[categorical_features]\n",
    "train_scaled['target'] = df_train['target']\n",
    "concat_scaled = concat_train_test(train_scaled, test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encoding for Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "def encode_label(dt_train, dt_test, categorical_columns_v2):\n",
    "    logger.info('#### 범주형 변수들을 대상으로 레이블인코딩을 진행해 주겠습니다.')\n",
    "    # 각 변수에 대한 LabelEncoder를 저장할 딕셔너리\n",
    "    label_encoders = {}\n",
    "\n",
    "    # Implement Label Encoding\n",
    "    for col in tqdm( categorical_columns_v2 ):\n",
    "        lbl = LabelEncoder()\n",
    "    \n",
    "        # Label-Encoding을 fit\n",
    "        lbl.fit( dt_train[col].astype(str) )\n",
    "        dt_train[col] = lbl.transform(dt_train[col].astype(str))\n",
    "        label_encoders[col] = lbl           # 나중에 후처리를 위해 레이블인코더를 저장해주겠습니다.\n",
    "\n",
    "        # Test 데이터에만 존재하는 새로 출현한 데이터를 신규 클래스로 추가해줍니다.\n",
    "        dt_test[col] = dt_test[col].astype(str)\n",
    "        for label in np.unique(dt_test[col]):\n",
    "            if label not in lbl.classes_: # unseen label 데이터인 경우\n",
    "                lbl.classes_ = np.append(lbl.classes_, label) # 미처리 시 ValueError발생하니 주의하세요!\n",
    "        dt_test[col] = lbl.transform(dt_test[col].astype(str))\n",
    "\n",
    "        dt_train.head(1)        # 레이블인코딩이 된 모습입니다.\n",
    "\n",
    "        assert dt_train.shape[1] == dt_test.shape[1]          # train/test dataset의 shape이 같은지 확인해주겠습니다.\n",
    "    return dt_train, dt_test, label_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded, test_encoded, label_encoders = encode_label(df_train[categorical_features], df_test[categorical_features] ,categorical_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded[numerical_features] = df_train[numerical_features]\n",
    "train_encoded['target'] = df_train['target']\n",
    "test_encoded[numerical_features] = df_test[numerical_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded.shape, test_encoded.shape\n",
    "\n",
    "df_encoded = concat_train_test(train_encoded, test_encoded)\n",
    "#df_encoded.to_csv(os.path.join(prep_path, 'df_encoded.csv'), index=False)\n",
    "\n",
    "df_encoded.isnull().sum()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
